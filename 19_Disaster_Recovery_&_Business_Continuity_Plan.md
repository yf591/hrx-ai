
# 19. Disaster Recovery & Business Continuity Plan (DR/BCP)

*バージョン: 1.0.0 | 最終更新日: 2025年5月14日*

## 目次

1.  [はじめに](#1-はじめに)
    *   1.1 本計画の目的
    *   1.2 対象範囲
    *   1.3 DR (災害復旧) と BCP (事業継続計画) の定義
    *   1.4 基本方針と目標
2.  [役割と責任](#2-役割と責任)
    *   2.1 DR/BCPコーディネーター (またはチーム)
    *   2.2 経営層
    *   2.3 各部門の責任者 (技術、運用、人事、法務、広報など)
    *   2.4 インシデント対応チーム (IRT) との連携
3.  [リスク評価とビジネスインパクト分析 (BIA)](#3-リスク評価とビジネスインパクト分析-bia)
    *   3.1 潜在的な災害シナリオの特定
        *   3.1.1 自然災害 (地震、洪水、火災など)
        *   3.1.2 技術的障害 (大規模なハードウェア障害、ソフトウェアバグ、クラウドプロバイダー障害)
        *   3.1.3 サイバー攻撃 (ランサムウェア、DDoS、データ侵害)
        *   3.1.4 人的エラー、パンデミック、サプライチェーン障害など
    *   3.2 ビジネスインパクト分析の実施
        *   3.2.1 重要業務プロセスとそれを支えるITシステムの特定
        *   3.2.2 各プロセスの停止がビジネスに与える影響 (財務的、評判、法的など) の評価
    *   3.3 RTO (目標復旧時間) と RPO (目標復旧ポイント) の設定
        *   3.3.1 HRX-AI全体および主要コンポーネントごとのRTO/RPO定義
4.  [災害復旧 (DR) 戦略](#4-災害復旧-dr-戦略)
    *   4.1 データバックアップとオフサイト保管 (`8. Deployment & Infrastructure Runbook` 6.1 と連携)
    *   4.2 DRサイトの選定と準備 (クラウドベースDR)
        *   4.2.1 コールドサイト、ウォームサイト、ホットサイトの比較と選択
        *   4.2.2 マルチリージョン/マルチゾーンアーキテクチャの活用 (Vercel, Railway, Firebase/Supabaseの機能)
    *   4.3 アプリケーションとデータのレプリケーション戦略
    *   4.4 ネットワーク復旧とDNSフェイルオーバー計画
    *   4.5 AIモデルと関連リソースのDR
5.  [事業継続計画 (BCP)](#5-事業継続計画-bcp)
    *   5.1 代替業務プロセスの確立
    *   5.2 重要従業員の代替と連絡体制
    *   5.3 リモートワーク体制への移行計画 (オフィス被災時など)
    *   5.4 サードパーティベンダーとの連携継続
    *   5.5 コミュニケーション計画 (従業員、顧客、ステークホルダー向け)
6.  [DR/BCP 発動と復旧手順](#6-drbcp-発動と復旧手順)
    *   6.1 発動基準と意思決定プロセス
    *   6.2 復旧チームの招集と役割分担
    *   6.3 データリストア手順 (`8. Deployment & Infrastructure Runbook` 6.2 と連携)
    *   6.4 システム切り替え手順 (DRサイトへのフェイルオーバー)
    *   6.5 動作検証とサービス再開
    *   6.6 通常運用への復帰 (フェイルバック) 手順
7.  [テストと訓練](#7-テストと訓練)
    *   7.1 DR/BCPテストの種類 (ウォークスルー、シミュレーション、フルテスト)
    *   7.2 テストの頻度とシナリオ
    *   7.3 テスト結果の評価と計画の見直し
    *   7.4 従業員向け訓練と意識向上
8.  [計画の維持と更新](#8-計画の維持と更新)
    *   8.1 定期的なレビューと改訂 (少なくとも年1回)
    *   8.2 システム変更、組織変更、リスク評価結果に応じた更新
    *   8.3 文書管理とアクセス性の確保
9.  [付録](#9-付録)
    *   9.1 緊急連絡先一覧 (再掲または詳細版)
    *   9.2 DR/BCP発動チェックリスト
    *   9.3 RTO/RPO一覧表 (システムコンポーネント別)

## 1. はじめに

*   **1.1 本計画の目的**
    本計画は、HRX-AIプラットフォームが重大な障害や災害 (以下、「インシデント」と総称) に見舞われた際に、事業活動への影響を最小限に抑え、定義された目標時間内に重要サービスを復旧し、事業継続を可能にすることを目的とします。`Development_Specification.md` 12.2 可用性・障害対応計画を補完し、エンタープライズ顧客が求める高い可用性を保証するための具体的な指針を記します。

*   **1.2 対象範囲**
    本計画は、HRX-AIプラットフォームを構成するすべてのITシステム、プロセス、データ、およびそれらをサポートする人員を対象とします。これには、フロントエンド、バックエンド、データベース、AIモデル、サードパーティ連携、および関連するインフラストラクチャが含まれます。

*   **1.3 DR (災害復旧) と BCP (事業継続計画) の定義**
    *   **災害復旧 (Disaster Recovery - DR)**
        *   インシデント発生後、ITシステムとインフラストラクチャを復旧し、機能を回復させるための技術的なプロセスと手順。主にITシステムとデータの復旧に焦点を当てる。
    *   **事業継続計画 (Business Continuity Plan - BCP)**
        *   インシデント発生中および発生後も、組織が重要業務を継続または迅速に再開できるようにするための戦略的かつ戦術的な計画。DRはBCPの一部。人的リソース、業務プロセス、コミュニケーションなども含む広範な計画。

*   **1.4 基本方針と目標**
    *   **人命の安全確保を最優先**
    *   **重要業務プロセスの継続** HRX-AIのコア機能 (顧客が依存している主要機能) の提供を可能な限り継続する。
    *   **データ損失の最小化** RPO (目標復旧ポイント) を達成し、重要なデータの損失を防ぐ。
    *   **サービス中断時間の最小化** RTO (目標復旧時間) を達成し、迅速にサービスを復旧する。
    *   **顧客およびステークホルダーへの適切なコミュニケーション** 透明性を保ち、信頼を維持する。
    *   **継続的な改善** 定期的なテストとレビューを通じて、DR/BCPの実効性を高める。

## 2. 役割と責任

*   **2.1 DR/BCPコーディネーター (またはチーム)**
    *   責任: DR/BCP全体の策定、維持、更新、テスト、訓練の主導。インシデント発生時のDR/BCP発動と復旧活動の調整・統括。経営層への報告。
    *   担当者例: CTO、CIO、または専任の事業継続マネージャー。
*   **2.2 経営層**
    *   責任: DR/BCP戦略の承認、必要なリソースの提供、インシデント発生時の最終的な意思決定 (例 DRサイトへの切り替え判断)。
*   **2.3 各部門の責任者**
    *   **技術部門 (開発、インフラ、運用)** DR戦略の技術的実装、システム復旧手順の実行、データリストア。
    *   **人事部門** 従業員の安否確認、代替業務体制の調整、従業員向けコミュニケーション。
    *   **法務・コンプライアンス部門** 法的義務の確認 (例 データ侵害通知)、契約上の責任確認。
    *   **広報部門** 社内外への公式な情報発信。
    *   **カスタマーサポート部門** 顧客からの問い合わせ対応、影響状況の伝達。
*   **2.4 インシデント対応チーム (IRT) との連携**
    `8. Deployment & Infrastructure Runbook` 5.2.6 および 7.2 で定義されるIRTと密接に連携。セキュリティインシデントがDR/BCP発動の引き金となる場合がある。

## 3. リスク評価とビジネスインパクト分析 (BIA)

*   **3.1 潜在的な災害シナリオの特定**
    HRX-AIの事業継続に影響を与える可能性のある災害シナリオを洗い出し、その発生可能性と影響度を評価します。
    *   **3.1.1 自然災害**
        *   例: 地震、台風/ハリケーン、洪水、大規模停電、火災 (データセンター、オフィス)。
        *   対策の方向性: 地理的に分散したインフラ (マルチリージョン/マルチゾーン)、クラウドプロバイダーのレジリエンス機能活用。
    *   **3.1.2 技術的障害**
        *   例: クラウドプロバイダーの大規模障害 (リージョン障害)、主要コンポーネント (データベース、認証システム) の致命的バグ、広範囲なネットワーク障害、ストレージ障害によるデータ破損。
        *   対策の方向性: 冗長構成、フェイルオーバーメカニズム、信頼性の高いクラウドサービスの選定、バックアップとリストア戦略。
    *   **3.1.3 サイバー攻撃**
        *   例: ランサムウェア攻撃によるシステム停止・データ暗号化、大規模DDoS攻撃によるサービス不能、重要なAIモデルやデータの破壊・改ざん。
        *   対策の方向性: `6. Security Implementation Guide` に基づく多層防御、オフラインバックアップ、インシデント対応計画。
    *   **3.1.4 人的エラー、パンデミック、サプライチェーン障害など**
        *   例: オペレーションミスによる大規模データ削除、重要担当者の長期離脱、主要サードパーティサービス (LLMプロバイダーなど) の長期供給停止。
        *   対策の方向性: 操作権限の厳格化、自動化、ドキュメント整備、代替担当者の育成、パンデミック対応計画、ベンダーリスク管理。

*   **3.2 ビジネスインパクト分析 (BIA) の実施**
    1.  **重要業務プロセスの特定**: HRX-AIが提供するサービスの中で、顧客ビジネスにとってクリティカルなプロセスを特定 (例 採用候補者スクリーニング、離職リスクアラート、給与計算連携データ出力など)。
    2.  **依存関係のマッピング**: 各重要業務プロセスが依存するITシステムコンポーネント (フロントエンド、API、DB、AIモデル、外部連携など) をマッピング。
    3.  **影響評価**: 各重要業務プロセスが停止した場合のビジネスへの影響を時間経過とともに評価。
        *   財務的影響 (収益損失、違約金など)
        *   顧客影響 (サービス利用不可、信頼失墜)
        *   運用影響 (業務停滞、手作業増加)
        *   評判影響 (ブランドイメージ低下)
        *   法的・契約的影響 (SLA違反、規制違反)
    4.  **最大許容停止時間 (MTPD / MAO)**: 各重要業務プロセスが停止しても許容できる最大時間を定義。これがRTO設定の基礎となる。

*   **3.3 RTO (目標復旧時間) と RPO (目標復旧ポイント) の設定**
    BIAの結果に基づき、HRX-AI全体および主要コンポーネントごとにRTOとRPOを定義します。
    *   **RTO (Recovery Time Objective)**
        *   インシデント発生からシステムまたはサービスが許容可能なレベルまで復旧し、業務を再開するまでの目標時間。
        *   例: HRX-AIコア機能 RTO = 4時間。重要顧客向けAPI RTO = 1時間。
    *   **RPO (Recovery Point Objective)**
        *   インシデント発生時に許容されるデータ損失の最大量 (時間で表現)。バックアップ頻度に依存。
        *   例: 主要データベース RPO = 15分。ファイルストレージ RPO = 1時間。
    *   これらは、ビジネス要件、コスト、技術的実現可能性のバランスを考慮して設定されます。エンタープライズ顧客向けには、より厳しいRTO/RPOがSLAで求められる場合があります (`Development_Specification.md` 6.1)。

## 4. 災害復旧 (DR) 戦略

RTO/RPOを達成するための具体的な技術的戦略です。`8. Deployment & Infrastructure Runbook` セクション6.3 ディザスタリカバリ (DR) 計画の概要を具体化します。

*   **4.1 データバックアップとオフサイト保管**
    *   「8. Deployment & Infrastructure Runbook」6.1 の戦略を確実に実施。
    *   **テスト**: バックアップデータからのリストアを定期的にテストし、RPOが満たせること、データが破損していないことを確認。
    *   **暗号化**: バックアップデータは強力に暗号化し、暗号鍵はバックアップデータとは別に安全に保管。

*   **4.2 DRサイトの選定と準備 (クラウドベースDR)**
    *   **方針**: プライマリリージョン (HRX-AIが通常稼働するクラウドリージョン) とは異なる地理的リージョンにDRサイトを準備します。
    *   **戦略の選択**:
        *   **バックアップとリストア (Backup and Restore)**: DR戦略の基本。バックアップデータをDRリージョンにコピーし、有事の際にDRリージョンでインフラを再構築してデータをリストア。RTOは比較的長くなるが、コストは低い。
        *   **パイロットライト (Pilot Light)**: DRリージョンにコアとなる最小限のインフラ (例 DBサーバーのレプリカ、小規模なアプリケーションサーバー群) を稼働させておき、データは常にレプリケート。有事の際にアプリケーションサーバーをスケールアウトし、DNSを切り替える。バックアップとリストアよりRTOは短縮。
        *   **ウォームスタンバイ (Warm Standby)**: DRリージョンにスケールダウンした本番環境のコピーを稼働させ、データは常にレプリケート。有事の際にスケールアップし、DNSを切り替える。パイロットライトよりRTOは短いがコストは増加。
        *   **マルチサイト (ホットスタンバイ / アクティブ・アクティブ)**: 複数のリージョンで同時にフルスケールの本番環境を稼働させ、ロードバランサーでトラフィックを分散。一方のリージョンがダウンしても、他方がシームレスに処理を引き継ぐ。RTOはほぼゼロに近いが、コストと運用複雑性は最も高い。
    *   **HRX-AIの選択**: 初期フェーズでは「バックアップとリストア」または「パイロットライト」から開始し、ビジネス成長と顧客要件に応じて「ウォームスタンバイ」や「マルチサイト」を検討。
    *   **インフラストラクチャ・アズ・コード (IaC)**: Terraform/Pulumi (`Development_Specification.md` 2.5) を使用してDRサイトのインフラ構成をコード化し、迅速かつ一貫性のある環境構築を可能にする。

*   **4.3 アプリケーションとデータのレプリケーション戦略**
    *   **データベース**
        *   Supabase (PostgreSQL): リージョン間リードレプリカ、または継続的アーカイブとDRリージョンでのPITR。
        *   Firebase Firestore: マルチリージョン構成を検討。手動でのクロスリージョンバックアップとリストア。
    *   **ファイルストレージ**
        *   Firebase Storage/Supabase Storage: クロスリージョンレプリケーション機能を利用。
    *   **ベクトルデータベース**: DR戦略は利用サービスに依存。スナップショットのクロスリージョンコピーとリストア、またはレプリケーション機能。
    *   **アプリケーションサーバー**: ステートレスに設計し、DRサイトでコンテナイメージやデプロイパッケージから迅速に再デプロイできるようにする。設定情報は環境変数や設定管理サービスで一元管理。

*   **4.4 ネットワーク復旧とDNSフェイルオーバー計画**
    *   **DNS**: Amazon Route 53, Google Cloud DNS, Cloudflare DNSなどのグローバルDNSサービスを利用。
    *   **ヘルスチェック**: プライマリサイトのエンドポイントに対して定期的なヘルスチェックを設定。
    *   **フェイルオーバーポリシー**: ヘルスチェック失敗時に、DNSレコードを自動または手動でDRサイトのIPアドレス/エンドポイントに切り替えるルーティングポリシーを設定 (例 Route 53のフェイルオーバールーティング)。
    *   **TTL (Time To Live)**: DNSレコードのTTL値を適切に設定し、切り替え時の伝播遅延を考慮 (障害発生の可能性が高まる前は短めに設定することも検討)。

*   **4.5 AIモデルと関連リソースのDR**
    *   **外部LLM API**: プロバイダー側のDR計画に依存。必要に応じて、異なるプロバイダーの代替モデルへの切り替え手順を準備。
    *   **カスタム訓練済みモデル**: モデルアーティファクト (重みファイルなど) と訓練コード、学習データ (またはその参照) をバージョン管理し、DRサイトでも再デプロイまたは再学習できる状態にしておく。モデルレジストリの利用。
    *   **ONNXモデル**: `.onnx` ファイルをDRサイトにレプリケート。
    *   **ベクトルデータベースのインデックス**: データと同様にレプリケートまたはDRサイトで再構築。

## 5. 事業継続計画 (BCP)

ITシステムの復旧だけでなく、ビジネスオペレーション全体の継続を目的とします。

*   **5.1 代替業務プロセスの確立**
    *   HRX-AIの重要機能が長時間利用できない場合に、手動または他のツールで行う代替業務プロセスを定義。
    *   例: 採用スクリーニングが停止した場合、履歴書の手動確認とスプレッドシートでの管理手順。
    *   必要な帳票や連絡リストを事前に準備。

*   **5.2 重要従業員の代替と連絡体制**
    *   DR/BCP発動時に重要な役割を担う従業員 (例 DR/BCPコーディネーター、主要技術者) が対応できない場合に備え、代替担当者を指名し、訓練。
    *   緊急連絡網 (電話、代替コミュニケーションツール) を整備し、最新の状態に維持。

*   **5.3 リモートワーク体制への移行計画**
    *   オフィスが被災した場合でも業務を継続できるよう、全従業員がリモートワーク可能な環境と体制を整備 (VPN、セキュアなデバイス、オンラインコラボレーションツール)。
    *   `Development_Specification.md` 3.3.3 ロケーション＆ワークスタイル分析 でリモート生産性も考慮。

*   **5.4 サードパーティベンダーとの連携継続**
    *   HRX-AIが依存する重要なサードパーティベンダー (クラウドプロバイダー、LLMプロバイダー、Stripeなど) のBCPを確認し、自社のBCPとの整合性を取る。
    *   ベンダー障害時の代替手段やSLAを確認。

*   **5.5 コミュニケーション計画**
    *   **従業員向け**: 安否確認、状況説明、業務指示、メンタルヘルスサポート。
    *   **顧客向け**: サービス停止/影響状況の通知、復旧見込み、代替手段の案内、問い合わせ窓口。
    *   **その他ステークホルダー (株主、メディア、規制当局など)**: 必要に応じた情報開示。
    *   事前にテンプレートや広報チャネル (ステータスページ、SNSなど) を準備。

## 6. DR/BCP 発動と復旧手順

*   **6.1 発動基準と意思決定プロセス**
    *   **発動基準**
        *   RTOを超えるシステム停止が予測される場合。
        *   データセンターやオフィスが物理的に機能しなくなった場合。
        *   広範囲なサイバー攻撃によりプライマリシステムが信頼できなくなった場合。
        *   BIAで定義された最大許容停止時間を超える可能性が高いと判断された場合。
    *   **意思決定**: DR/BCPコーディネーターが状況を評価し、経営層にDR/BCP発動を勧告。経営層が最終決定。

*   **6.2 復旧チームの招集と役割分担**
    *   DR/BCP発動決定後、事前に定義された復旧チームメンバーを緊急招集。
    *   各メンバーの役割 (インフラ復旧、アプリ復旧、データリストア、通信、顧客対応など) を再確認。

*   **6.3 データリストア手順**
    *   「8. Deployment & Infrastructure Runbook」6.2 の手順に従い、DRサイトで最新かつ整合性の取れたバックアップからデータをリストア。
    *   RPOを考慮し、どの時点のバックアップを使用するか決定。
    *   リストア後のデータ検証を必ず実施。

*   **6.4 システム切り替え手順 (DRサイトへのフェイルオーバー)**
    1.  DRサイトのインフラが完全にプロビジョニングされ、アプリケーションがデプロイされていることを確認。
    2.  データリストアと検証が完了していることを確認。
    3.  DRサイトのシステムに対して最終的なヘルスチェックとスモークテストを実施。
    4.  DNSレコードをDRサイトのエンドポイントに切り替え。伝播時間を考慮。
    5.  プライマリサイトからのトラフィックがDRサイトに完全に移行したことを確認。
    6.  プライマリサイトへのアクセスを遮断 (さらなるデータ不整合を防ぐため)。

*   **6.5 動作検証とサービス再開**
    *   DRサイトでHRX-AIの主要機能が正常に動作することを包括的にテスト。
    *   段階的にユーザーアクセスを許可し、サービスの全面再開を宣言。
    *   再開後もシステム監視を強化。

*   **6.6 通常運用への復帰 (フェイルバック) 手順**
    *   プライマリサイトが復旧し、安全が確認された後、DRサイトからプライマリサイトへ運用を戻す計画。
    *   **計画**: フェイルバックのタイミング、手順、リスクを評価。
    *   **データ同期**: DRサイトで発生した変更データをプライマリサイトに同期 (これが最も複雑な部分であり、同期方向やコンフリクト解決策を事前に設計しておく必要がある)。
    *   **システム切り替え**: DNSレコードをプライマリサイトに戻す。
    *   **検証**: プライマリサイトでの動作確認。
    *   **注意**: フェイルバックはフェイルオーバーよりも計画的かつ慎重に実施。場合によってはDRサイトでの運用を継続する判断もあり得る。

## 7. テストと訓練

*   **7.1 DR/BCPテストの種類**
    *   **ウォークスルー (書類確認)**: 計画書を関係者で読み合わせ、手順や役割分担の理解度を確認。年1-2回。
    *   **シミュレーション (卓上演習)**: 特定の災害シナリオを想定し、各チームがどのように対応するかを机上でシミュレート。年1回。
    *   **コンポーネントテスト**: 個別の復旧手順 (例 DBリストア、特定サービスのDRサイト起動) をテスト。半期に1回。
    *   **フルテスト (DR訓練)**: 実際にDRサイトへシステムを切り替える総合的な訓練。年1回または隔年。影響範囲を考慮し、週末や夜間に実施。

*   **7.2 テストの頻度とシナリオ**
    *   上記参照。シナリオは3.1で特定したものをローテーションで実施。
*   **7.3 テスト結果の評価と計画の見直し**
    *   テストで明らかになった問題点、手順の不備、RTO/RPO達成状況などを記録。
    *   結果を分析し、DR/BCPおよび関連手順書を改善。
*   **7.4 従業員向け訓練と意識向上**
    *   全従業員に対してBCPの概要と自身の役割 (安否確認への応答など) を周知。
    *   復旧チームメンバーに対しては、より専門的な技術訓練を実施。

## 8. 計画の維持と更新

*   **8.1 定期的なレビューと改訂**
    *   DR/BCPは、少なくとも年に1回、または以下のイベント発生時にレビューし、必要に応じて改訂します。
*   **8.2 システム変更、組織変更、リスク評価結果に応じた更新**
    *   大規模なシステムアーキテクチャ変更
    *   新しいテクノロジーやサービスの導入
    *   組織体制やキーパーソンの変更
    *   新たな脅威や脆弱性の認識
    *   BIAの結果更新
*   **8.3 文書管理とアクセス性の確保**
    *   DR/BCP文書はバージョン管理され、常に最新版が指定された場所 (例 社内ポータル、ドキュメント管理システム) に保管され、権限のある担当者がいつでもアクセスできるようにします。
    *   オフラインでも参照できるよう、主要部分のハードコピーやローカルコピーも準備。

## 9. 付録

*   **9.1 緊急連絡先一覧 (再掲または詳細版)**
    *   DR/BCPコーディネーター、復旧チームメンバー、経営層、主要ベンダーサポート、関連機関などの最新連絡先リスト。複数の連絡手段を記載。
*   **9.2 DR/BCP発動チェックリスト**
    *   インシデント発生からDR/BCP発動決定、チーム招集、初期対応までのステップをまとめたチェックリスト。
*   **9.3 RTO/RPO一覧表 (システムコンポーネント別)**
    *   HRX-AIを構成する主要なITコンポーネント (例 フロントエンド、APIサーバー群、認証DB、メイン業務DB、ファイルストレージ、AI推論サービスA、AI推論サービスBなど) ごとに設定されたRTOとRPOの一覧。

---